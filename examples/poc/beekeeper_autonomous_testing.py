"""
BeeKeeperè‡ªå¾‹çš„ãƒ†ã‚¹ãƒˆè‡ªå‹•ç”ŸæˆPoC

æ­£ã—ã„ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼š
1. BeeKeeperï¼ˆäººé–“ï¼‰ãŒãƒ†ã‚¹ãƒˆç”Ÿæˆç›®çš„ãƒ»å“è³ªåŸºæº–æŠ•å…¥
2. Queen/Developeré–“ã§è‡ªå¾‹çš„å”èª¿ãƒ»ãƒ†ã‚¹ãƒˆç”Ÿæˆ
3. æˆæœç‰©ï¼ˆç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚¹ãƒˆï¼‰ã‚’äººé–“ãŒå—é ˜

Usage:
    python examples/poc/beekeeper_autonomous_testing.py
"""

import ast
import asyncio
import json
import logging
import subprocess
from datetime import datetime
from pathlib import Path
from typing import Any

from comb.api import CombAPI
from comb.message_router import MessagePriority, MessageType


class BeeKeeperTestingInput:
    """BeeKeeperï¼ˆäººé–“ï¼‰ã‹ã‚‰ã®ãƒ†ã‚¹ãƒˆç”Ÿæˆå…¥åŠ›"""

    def __init__(self):
        self.testing_requests = {}

    def submit_testing_request(
        self,
        objective: str,
        template: str,
        target_files: list[str] = None,
        coverage_targets: dict[str, Any] = None,
        test_types: list[str] = None,
    ) -> str:
        """ãƒ†ã‚¹ãƒˆç”Ÿæˆè¦æ±‚æŠ•å…¥"""

        request_id = f"testing_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

        self.testing_requests[request_id] = {
            "objective": objective,
            "template": template,
            "target_files": target_files or [],
            "coverage_targets": coverage_targets
            or {
                "line_coverage": 85,
                "branch_coverage": 80,
                "function_coverage": 90,
                "edge_cases_coverage": 75,
            },
            "test_types": test_types or ["unit", "integration", "edge_case"],
            "timestamp": datetime.now().isoformat(),
            "status": "submitted",
        }

        print(f"ğŸ BeeKeeper Testing Request: {request_id}")
        print(f"ğŸ“‹ Objective: {objective}")
        print(f"ğŸ“ Template: {template}")
        print(f"ğŸ¯ Coverage Targets: {coverage_targets}")
        print(f"ğŸ§ª Test Types: {test_types}")

        return request_id

    def get_request_data(self, request_id: str) -> dict[str, Any]:
        """è¦æ±‚ãƒ‡ãƒ¼ã‚¿å–å¾—"""
        return self.testing_requests.get(request_id, {})


class QueenTestingCoordinator:
    """Queen Worker - ãƒ†ã‚¹ãƒˆç”Ÿæˆå…¨ä½“èª¿æ•´"""

    def __init__(self):
        self.comb_api = CombAPI("queen")
        self.logger = logging.getLogger("queen_testing")
        self.active_testing_projects = {}
        self.conversation_history = []

    async def receive_beekeeper_request(
        self, request_id: str, request_data: dict[str, Any]
    ) -> None:
        """BeeKeeperãƒ†ã‚¹ãƒˆç”Ÿæˆè¦æ±‚å—ä¿¡"""

        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåˆæœŸåŒ–
        project_id = f"testing_{request_id}"
        self.active_testing_projects[project_id] = {
            "request_id": request_id,
            "objective": request_data["objective"],
            "template": request_data["template"],
            "target_files": request_data["target_files"],
            "coverage_targets": request_data["coverage_targets"],
            "test_types": request_data["test_types"],
            "status": "analyzing",
            "created_at": datetime.now().isoformat(),
        }

        # Work Logé–‹å§‹
        task_id = self.comb_api.start_task(
            f"BeeKeeper Testing: {request_data['objective']}",
            "beekeeper_testing",
            f"Starting test generation project: {request_data['objective']}",
            workers=["queen", "developer"],
        )

        self.active_testing_projects[project_id]["task_id"] = task_id

        # ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹åˆ†æ
        analysis = await self._analyze_testable_code(request_data)
        self.active_testing_projects[project_id]["initial_analysis"] = analysis

        # ãƒ†ã‚¹ãƒˆç”Ÿæˆæˆ¦ç•¥ç­–å®š
        strategy = await self._develop_testing_strategy(request_data, analysis)
        self.active_testing_projects[project_id]["strategy"] = strategy

        # Developer Workerã«åˆæœŸæŒ‡ç¤º
        await self._send_initial_testing_instructions(project_id, strategy)

        self.logger.info(f"ğŸ‘‘ Queen: Started testing project {project_id}")

    async def _analyze_testable_code(
        self, request_data: dict[str, Any]
    ) -> dict[str, Any]:
        """ãƒ†ã‚¹ãƒˆå¯¾è±¡ã‚³ãƒ¼ãƒ‰åˆ†æ"""

        analysis = {
            "total_python_files": 0,
            "testable_functions": [],
            "testable_classes": [],
            "current_test_coverage": 0,
            "missing_test_files": [],
            "complexity_analysis": {},
            "edge_case_opportunities": [],
        }

        try:
            # Pythonãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢
            python_files = list(Path(".").rglob("*.py"))
            python_files = [
                f
                for f in python_files
                if not any(part.startswith(".") for part in f.parts)
                and "test_" not in f.name
                and "__pycache__" not in str(f)
            ]

            analysis["total_python_files"] = len(python_files)

            # é–¢æ•°ãƒ»ã‚¯ãƒ©ã‚¹åˆ†æ
            for file_path in python_files[:15]:  # æœ€åˆã®15ãƒ•ã‚¡ã‚¤ãƒ«
                try:
                    with open(file_path, encoding="utf-8") as f:
                        tree = ast.parse(f.read())

                    file_functions = []
                    file_classes = []

                    for node in ast.walk(tree):
                        if isinstance(node, ast.FunctionDef):
                            if not node.name.startswith("_"):  # ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆé–¢æ•°é™¤å¤–
                                func_info = {
                                    "name": node.name,
                                    "file": str(file_path),
                                    "line": node.lineno,
                                    "args": [arg.arg for arg in node.args.args],
                                    "complexity": await self._calculate_complexity(
                                        node
                                    ),
                                    "has_test": await self._check_existing_test(
                                        file_path, node.name
                                    ),
                                }
                                file_functions.append(func_info)

                        elif isinstance(node, ast.ClassDef):
                            class_info = {
                                "name": node.name,
                                "file": str(file_path),
                                "line": node.lineno,
                                "methods": [],
                                "has_test": await self._check_existing_test(
                                    file_path, node.name
                                ),
                            }

                            # ãƒ¡ã‚½ãƒƒãƒ‰åˆ†æ
                            for item in node.body:
                                if isinstance(item, ast.FunctionDef):
                                    class_info["methods"].append(
                                        {
                                            "name": item.name,
                                            "args": [arg.arg for arg in item.args.args],
                                            "complexity": await self._calculate_complexity(
                                                item
                                            ),
                                        }
                                    )

                            file_classes.append(class_info)

                    analysis["testable_functions"].extend(file_functions)
                    analysis["testable_classes"].extend(file_classes)

                    # å¯¾å¿œã™ã‚‹ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
                    test_file = Path(f"tests/test_{file_path.stem}.py")
                    if not test_file.exists():
                        analysis["missing_test_files"].append(str(file_path))

                except Exception as e:
                    self.logger.warning(f"Failed to analyze {file_path}: {e}")

            # ç¾åœ¨ã®ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸å–å¾—
            analysis["current_test_coverage"] = await self._get_current_coverage()

            # ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹æ©Ÿä¼šåˆ†æ
            analysis[
                "edge_case_opportunities"
            ] = await self._identify_edge_case_opportunities(
                analysis["testable_functions"]
            )

            self.logger.info(
                f"ğŸ‘‘ Queen: Code analysis completed - "
                f"Functions: {len(analysis['testable_functions'])}, "
                f"Classes: {len(analysis['testable_classes'])}, "
                f"Coverage: {analysis['current_test_coverage']:.1f}%"
            )

        except Exception as e:
            self.logger.error(f"Code analysis error: {e}")

        return analysis

    async def _calculate_complexity(self, node: ast.FunctionDef) -> int:
        """ã‚µã‚¤ã‚¯ãƒ­ãƒãƒ†ã‚£ãƒƒã‚¯è¤‡é›‘åº¦è¨ˆç®—"""
        complexity = 1

        for child in ast.walk(node):
            if isinstance(child, ast.If | ast.While | ast.For | ast.AsyncFor):
                complexity += 1
            elif isinstance(child, ast.ExceptHandler):
                complexity += 1
            elif isinstance(child, ast.BoolOp):
                complexity += len(child.values) - 1

        return complexity

    async def _check_existing_test(self, file_path: Path, name: str) -> bool:
        """æ—¢å­˜ãƒ†ã‚¹ãƒˆã®å­˜åœ¨ç¢ºèª"""
        test_file = Path(f"tests/test_{file_path.stem}.py")

        if not test_file.exists():
            return False

        try:
            with open(test_file, encoding="utf-8") as f:
                content = f.read()
                return f"test_{name}" in content
        except Exception:
            return False

    async def _get_current_coverage(self) -> float:
        """ç¾åœ¨ã®ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸å–å¾—"""
        try:
            result = subprocess.run(
                ["python", "-m", "pytest", "--cov=.", "--cov-report=json", "--quiet"],
                capture_output=True,
                text=True,
                timeout=60,
            )

            if result.returncode == 0:
                coverage_file = Path("coverage.json")
                if coverage_file.exists():
                    with open(coverage_file) as f:
                        data = json.load(f)
                    coverage_file.unlink()

                    return data.get("totals", {}).get("percent_covered", 0)

            return 0.0

        except Exception:
            return 0.0

    async def _identify_edge_case_opportunities(
        self, functions: list[dict[str, Any]]
    ) -> list[dict[str, Any]]:
        """ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹æ©Ÿä¼šç‰¹å®š"""
        opportunities = []

        for func in functions:
            args = func.get("args", [])
            complexity = func.get("complexity", 1)

            # è¤‡é›‘åº¦ãŒé«˜ã„é–¢æ•°
            if complexity > 5:
                opportunities.append(
                    {
                        "function": func["name"],
                        "type": "high_complexity",
                        "priority": "high",
                        "description": f"Function {func['name']} has complexity {complexity}",
                    }
                )

            # è¤‡æ•°å¼•æ•°ã‚’æŒã¤é–¢æ•°
            if len(args) > 3:
                opportunities.append(
                    {
                        "function": func["name"],
                        "type": "multiple_parameters",
                        "priority": "medium",
                        "description": f"Function {func['name']} has {len(args)} parameters",
                    }
                )

        return opportunities

    async def _develop_testing_strategy(
        self, request_data: dict[str, Any], analysis: dict[str, Any]
    ) -> dict[str, Any]:
        """ãƒ†ã‚¹ãƒˆç”Ÿæˆæˆ¦ç•¥ç­–å®š"""

        coverage_targets = request_data["coverage_targets"]
        test_types = request_data["test_types"]
        current_coverage = analysis["current_test_coverage"]

        strategy = {
            "current_coverage": current_coverage,
            "target_coverage": coverage_targets.get("line_coverage", 85),
            "coverage_gap": coverage_targets.get("line_coverage", 85)
            - current_coverage,
            "priority_actions": [],
        }

        # å„ªå…ˆã‚¢ã‚¯ã‚·ãƒ§ãƒ³æ±ºå®š
        if len(analysis["missing_test_files"]) > 0:
            strategy["priority_actions"].append(
                {
                    "action": "create_missing_test_files",
                    "files_count": len(analysis["missing_test_files"]),
                    "priority": "high",
                }
            )

        if "unit" in test_types:
            untested_functions = [
                f for f in analysis["testable_functions"] if not f["has_test"]
            ]
            if untested_functions:
                strategy["priority_actions"].append(
                    {
                        "action": "generate_unit_tests",
                        "functions_count": len(untested_functions),
                        "priority": "high",
                    }
                )

        if "edge_case" in test_types and analysis["edge_case_opportunities"]:
            strategy["priority_actions"].append(
                {
                    "action": "generate_edge_case_tests",
                    "opportunities_count": len(analysis["edge_case_opportunities"]),
                    "priority": "medium",
                }
            )

        if "integration" in test_types:
            strategy["priority_actions"].append(
                {
                    "action": "generate_integration_tests",
                    "classes_count": len(analysis["testable_classes"]),
                    "priority": "low",
                }
            )

        # å®Ÿè¡Œè¨ˆç”»ä½œæˆ
        strategy["execution_plan"] = []
        for i, action in enumerate(strategy["priority_actions"], 1):
            strategy["execution_plan"].append(
                {
                    "step": i,
                    "action": action["action"],
                    "description": self._get_testing_action_description(action),
                    "priority": action["priority"],
                }
            )

        # Work Logè¨˜éŒ²
        self.comb_api.add_technical_decision(
            "Testing Strategy",
            f"Planned {len(strategy['priority_actions'])} testing actions to improve coverage from {current_coverage:.1f}% to {coverage_targets.get('line_coverage', 85)}%",
            ["Manual testing", "Alternative testing frameworks"],
        )

        return strategy

    def _get_testing_action_description(self, action: dict[str, Any]) -> str:
        """ãƒ†ã‚¹ãƒˆã‚¢ã‚¯ã‚·ãƒ§ãƒ³èª¬æ˜ç”Ÿæˆ"""
        action_type = action["action"]

        if action_type == "create_missing_test_files":
            return f"Create {action['files_count']} missing test files"
        elif action_type == "generate_unit_tests":
            return f"Generate unit tests for {action['functions_count']} functions"
        elif action_type == "generate_edge_case_tests":
            return f"Generate edge case tests for {action['opportunities_count']} opportunities"
        elif action_type == "generate_integration_tests":
            return f"Generate integration tests for {action['classes_count']} classes"
        else:
            return f"Execute {action_type}"

    async def _send_initial_testing_instructions(
        self, project_id: str, strategy: dict[str, Any]
    ) -> None:
        """Developer Workerã«åˆæœŸãƒ†ã‚¹ãƒˆç”ŸæˆæŒ‡ç¤º"""

        project = self.active_testing_projects[project_id]

        instructions = {
            "action": "start_testing",
            "project_id": project_id,
            "objective": project["objective"],
            "template": project["template"],
            "coverage_targets": project["coverage_targets"],
            "test_types": project["test_types"],
            "strategy": strategy,
            "execution_plan": strategy["execution_plan"],
            "queen_message": f"ğŸ‘‘ Starting test generation project: {project['objective']}",
        }

        success = self.comb_api.send_message(
            to_worker="developer",
            content=instructions,
            message_type=MessageType.REQUEST,
            priority=MessagePriority.HIGH,
        )

        if success:
            self.conversation_history.append(
                {
                    "from": "queen",
                    "to": "developer",
                    "type": "testing_instructions",
                    "content": instructions,
                    "timestamp": datetime.now().isoformat(),
                }
            )

            self.logger.info(f"ğŸ‘‘ Queen: Sent testing instructions for {project_id}")

    async def monitor_testing_progress(self, project_id: str) -> None:
        """ãƒ†ã‚¹ãƒˆç”Ÿæˆé€²æ—ç›£è¦–"""

        project = self.active_testing_projects.get(project_id)
        if not project:
            return

        # Developer Workerã‹ã‚‰ã®å¿œç­”ç›£è¦–
        messages = self.comb_api.receive_messages()

        for message in messages:
            if message.from_worker == "developer":
                await self._handle_developer_testing_response(project_id, message)

    async def _handle_developer_testing_response(
        self, project_id: str, message
    ) -> None:
        """Developer Workerãƒ†ã‚¹ãƒˆç”Ÿæˆå¿œç­”å‡¦ç†"""

        content = message.content
        response_type = content.get("type", "unknown")

        # ä¼šè©±å±¥æ­´è¨˜éŒ²
        self.conversation_history.append(
            {
                "from": "developer",
                "to": "queen",
                "type": response_type,
                "content": content,
                "timestamp": datetime.now().isoformat(),
            }
        )

        # å¿œç­”ç¨®åˆ¥å‡¦ç†
        if response_type == "testing_progress":
            await self._handle_testing_progress(project_id, content)
        elif response_type == "test_generation_result":
            await self._handle_test_generation_result(project_id, content)
        elif response_type == "testing_question":
            await self._handle_testing_question(project_id, content)
        elif response_type == "testing_completion":
            await self._handle_testing_completion(project_id, content)
        elif response_type == "testing_issue":
            await self._handle_testing_issue(project_id, content)

        self.logger.info(f"ğŸ‘‘ Queen: Handled {response_type} from developer")

    async def _handle_testing_progress(
        self, project_id: str, content: dict[str, Any]
    ) -> None:
        """ãƒ†ã‚¹ãƒˆç”Ÿæˆé€²æ—å‡¦ç†"""

        progress = content.get("progress", {})
        current_action = progress.get("current_action", "unknown")
        completion_percentage = progress.get("completion_percentage", 0)

        # Work Logè¨˜éŒ²
        self.comb_api.add_progress(
            f"Testing Progress: {current_action}",
            f"Progress: {completion_percentage}% - {progress.get('description', '')}",
        )

        # å¿…è¦ã«å¿œã˜ã¦è¿½åŠ æŒ‡ç¤º
        if progress.get("needs_guidance", False):
            guidance = await self._provide_testing_guidance(project_id, progress)

            guidance_message = {
                "action": "testing_guidance",
                "project_id": project_id,
                "guidance": guidance,
                "queen_message": f"ğŸ‘‘ Guidance for {current_action}: {guidance}",
            }

            self.comb_api.send_message(
                to_worker="developer",
                content=guidance_message,
                message_type=MessageType.RESPONSE,
                priority=MessagePriority.MEDIUM,
            )

    async def _handle_test_generation_result(
        self, project_id: str, content: dict[str, Any]
    ) -> None:
        """ãƒ†ã‚¹ãƒˆç”Ÿæˆçµæœå‡¦ç†"""

        result = content.get("result", {})
        test_type = result.get("test_type", "unknown")
        tests_generated = result.get("tests_generated", 0)
        coverage_improvement = result.get("coverage_improvement", 0)

        # Work Logè¨˜éŒ²
        self.comb_api.add_progress(
            f"Test Generation Result: {test_type}",
            f"Generated {tests_generated} tests, coverage improvement: +{coverage_improvement:.1f}%",
        )

        # æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³æŒ‡ç¤º
        project = self.active_testing_projects[project_id]
        strategy = project.get("strategy", {})

        next_action = await self._determine_next_testing_action(
            project_id, result, strategy
        )

        if next_action:
            next_instruction = {
                "action": "continue_testing",
                "project_id": project_id,
                "next_action": next_action,
                "queen_message": f"ğŸ‘‘ Great progress! Next: {next_action['description']}",
            }

            self.comb_api.send_message(
                to_worker="developer",
                content=next_instruction,
                message_type=MessageType.REQUEST,
                priority=MessagePriority.HIGH,
            )

    async def _handle_testing_question(
        self, project_id: str, content: dict[str, Any]
    ) -> None:
        """ãƒ†ã‚¹ãƒˆç”Ÿæˆè³ªå•å‡¦ç†"""

        question = content.get("question", "")
        context = content.get("context", {})

        # è³ªå•åˆ†æã¨å›ç­”
        answer = await self._generate_testing_answer(project_id, question, context)

        answer_message = {
            "action": "testing_answer",
            "project_id": project_id,
            "question": question,
            "answer": answer,
            "queen_message": f"ğŸ‘‘ Answer: {answer}",
        }

        self.comb_api.send_message(
            to_worker="developer",
            content=answer_message,
            message_type=MessageType.RESPONSE,
            priority=MessagePriority.HIGH,
        )

    async def _handle_testing_completion(
        self, project_id: str, content: dict[str, Any]
    ) -> None:
        """ãƒ†ã‚¹ãƒˆç”Ÿæˆå®Œäº†å‡¦ç†"""

        final_results = content.get("final_results", {})
        test_summary = content.get("test_summary", {})

        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå®Œäº†å‡¦ç†
        project = self.active_testing_projects[project_id]
        project["status"] = "completed"
        project["final_results"] = final_results
        project["test_summary"] = test_summary
        project["completed_at"] = datetime.now().isoformat()

        # Work Logå®Œäº†
        task_id = project.get("task_id")
        if task_id:
            coverage_improvement = final_results.get("coverage_improvement", 0)
            tests_generated = final_results.get("total_tests_generated", 0)

            self.comb_api.complete_task(
                f"Testing completed successfully. Generated {tests_generated} tests, "
                f"coverage improvement: +{coverage_improvement:.1f}%"
            )

        # æˆæœç‰©ã‚’BeeKeeperå‘ã‘ã«å‡ºåŠ›
        await self._output_testing_deliverables(project_id, final_results, test_summary)

        self.logger.info(f"ğŸ‘‘ Queen: Testing project {project_id} completed")

    async def _handle_testing_issue(
        self, project_id: str, content: dict[str, Any]
    ) -> None:
        """ãƒ†ã‚¹ãƒˆç”Ÿæˆèª²é¡Œå‡¦ç†"""

        issue = content.get("issue", "")
        severity = content.get("severity", "medium")

        # èª²é¡Œåˆ†æã¨è§£æ±ºç­–
        solution = await self._analyze_testing_issue(project_id, issue, severity)

        solution_message = {
            "action": "testing_solution",
            "project_id": project_id,
            "issue": issue,
            "solution": solution,
            "queen_message": f"ğŸ‘‘ Solution for {issue}: {solution}",
        }

        self.comb_api.send_message(
            to_worker="developer",
            content=solution_message,
            message_type=MessageType.RESPONSE,
            priority=MessagePriority.HIGH,
        )

        # Work Logè¨˜éŒ²
        self.comb_api.add_challenge(issue, solution)

    async def _provide_testing_guidance(
        self, project_id: str, progress: dict[str, Any]
    ) -> str:
        """ãƒ†ã‚¹ãƒˆç”ŸæˆæŒ‡å°"""
        current_action = progress.get("current_action", "unknown")

        guidance_map = {
            "create_missing_test_files": "Create basic test file structure with proper imports and test class setup.",
            "generate_unit_tests": "Focus on testing individual functions in isolation. Use mocking for dependencies.",
            "generate_edge_case_tests": "Test boundary conditions, null values, empty inputs, and error conditions.",
            "generate_integration_tests": "Test class interactions and end-to-end workflows.",
        }

        return guidance_map.get(
            current_action,
            "Follow the testing template and focus on comprehensive coverage",
        )

    async def _determine_next_testing_action(
        self, project_id: str, result: dict[str, Any], strategy: dict[str, Any]
    ) -> dict[str, Any] | None:
        """æ¬¡ã®ãƒ†ã‚¹ãƒˆã‚¢ã‚¯ã‚·ãƒ§ãƒ³æ±ºå®š"""

        execution_plan = strategy.get("execution_plan", [])
        completed_actions = result.get("completed_actions", [])

        # æœªå®Œäº†ã‚¢ã‚¯ã‚·ãƒ§ãƒ³æ¢ç´¢
        for action in execution_plan:
            if action["action"] not in completed_actions:
                return action

        return None

    async def _generate_testing_answer(
        self, project_id: str, question: str, context: dict[str, Any]
    ) -> str:
        """ãƒ†ã‚¹ãƒˆç”Ÿæˆå›ç­”ç”Ÿæˆ"""

        project = self.active_testing_projects[project_id]

        if "mock" in question.lower():
            return "Use pytest fixtures and unittest.mock for mocking dependencies. Mock external services and databases."
        elif "assertion" in question.lower():
            return "Use appropriate assertions: assertEqual, assertTrue, assertIn, etc. Be specific about expected values."
        elif "edge case" in question.lower():
            return "Test with None, empty strings, zero, negative numbers, and boundary values."
        elif "fixture" in question.lower():
            return "Use pytest fixtures for test setup. Create reusable fixtures for common test data."
        else:
            return f"Refer to the testing objective: {project['objective']} and follow the established template."

    async def _analyze_testing_issue(
        self, project_id: str, issue: str, severity: str
    ) -> str:
        """ãƒ†ã‚¹ãƒˆç”Ÿæˆèª²é¡Œåˆ†æ"""

        if "import" in issue.lower():
            return "Check import paths and ensure the module is in PYTHONPATH. Use relative imports if needed."
        elif "fixture" in issue.lower():
            return (
                "Ensure fixtures are properly defined with @pytest.fixture decorator."
            )
        elif "mock" in issue.lower():
            return "Use patch decorator or context manager for proper mocking setup."
        elif "assertion" in issue.lower():
            return (
                "Use appropriate assertion methods and check expected vs actual values."
            )
        else:
            return f"For '{issue}', check pytest documentation and ensure proper test structure."

    async def _output_testing_deliverables(
        self,
        project_id: str,
        final_results: dict[str, Any],
        test_summary: dict[str, Any],
    ) -> None:
        """ãƒ†ã‚¹ãƒˆç”Ÿæˆæˆæœç‰©å‡ºåŠ›"""

        project = self.active_testing_projects[project_id]

        # æˆæœç‰©ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        output_dir = Path(f".hive/honey/beekeeper_testing/{project_id}")
        output_dir.mkdir(parents=True, exist_ok=True)

        # æˆæœç‰©ã‚µãƒãƒªãƒ¼
        deliverables = {
            "project_id": project_id,
            "beekeeper_objective": project["objective"],
            "template_used": project["template"],
            "initial_analysis": project["initial_analysis"],
            "final_results": final_results,
            "test_summary": test_summary,
            "conversation_history": self.conversation_history,
            "completion_time": datetime.now().isoformat(),
            "summary": {
                "coverage_improvement": f"{project['initial_analysis']['current_test_coverage']:.1f}% â†’ {final_results.get('final_coverage', 0):.1f}%",
                "total_tests_generated": final_results.get("total_tests_generated", 0),
                "conversation_turns": len(self.conversation_history),
            },
        }

        # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        summary_file = output_dir / "testing_summary.json"
        with open(summary_file, "w", encoding="utf-8") as f:
            json.dump(deliverables, f, ensure_ascii=False, indent=2)

        # ãƒ†ã‚¹ãƒˆç”Ÿæˆãƒ¬ãƒãƒ¼ãƒˆ
        test_report = output_dir / "test_generation_report.md"
        with open(test_report, "w", encoding="utf-8") as f:
            f.write("# Test Generation Report\n\n")
            f.write(f"**Project:** {project['objective']}\n")
            f.write(
                f"**Completed:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
            )

            f.write("## Test Generation Results\n\n")
            f.write(
                f"- **Total Tests Generated**: {final_results.get('total_tests_generated', 0)}\n"
            )
            f.write(
                f"- **Coverage Improvement**: {project['initial_analysis']['current_test_coverage']:.1f}% â†’ {final_results.get('final_coverage', 0):.1f}%\n"
            )
            f.write(
                f"- **Test Files Created**: {final_results.get('test_files_created', 0)}\n"
            )

            f.write("\n## Test Types Generated\n\n")
            for test_type, count in test_summary.items():
                f.write(f"- **{test_type}**: {count} tests\n")

            f.write("\n## Conversation Summary\n\n")
            f.write(
                f"Total Queen-Developer exchanges: {len(self.conversation_history)}\n"
            )

        print(f"ğŸ¯ Testing deliverables saved to: {output_dir}")
        print(f"ğŸ“‹ Summary: {summary_file}")
        print(f"ğŸ“„ Report: {test_report}")


class DeveloperTestingWorker:
    """Developer Worker - ãƒ†ã‚¹ãƒˆç”Ÿæˆå®Ÿè£…"""

    def __init__(self):
        self.comb_api = CombAPI("developer")
        self.logger = logging.getLogger("developer_testing")
        self.active_testing_projects = {}
        self.test_generation_progress = {}

    async def start_testing_monitoring(self) -> None:
        """ãƒ†ã‚¹ãƒˆç”Ÿæˆç›£è¦–é–‹å§‹"""
        self.logger.info("ğŸ’» Developer: Starting testing monitoring")

        while True:
            try:
                # Queen ã‹ã‚‰ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç¢ºèª
                messages = self.comb_api.receive_messages()

                for message in messages:
                    if message.from_worker == "queen":
                        await self._handle_queen_testing_message(message)

                await asyncio.sleep(5)  # 5ç§’é–“éš”

            except Exception as e:
                self.logger.error(f"ğŸ’» Developer: Error in monitoring: {e}")
                await asyncio.sleep(10)

    async def _handle_queen_testing_message(self, message) -> None:
        """Queen ãƒ†ã‚¹ãƒˆç”Ÿæˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å‡¦ç†"""
        content = message.content
        action = content.get("action", "unknown")

        if action == "start_testing":
            await self._start_testing_project(content)
        elif action == "testing_guidance":
            await self._receive_testing_guidance(content)
        elif action == "continue_testing":
            await self._continue_testing(content)
        elif action == "testing_answer":
            await self._receive_testing_answer(content)
        elif action == "testing_solution":
            await self._receive_testing_solution(content)

        self.logger.info(f"ğŸ’» Developer: Handled {action} from Queen")

    async def _start_testing_project(self, content: dict[str, Any]) -> None:
        """ãƒ†ã‚¹ãƒˆç”Ÿæˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆé–‹å§‹"""
        project_id = content["project_id"]
        objective = content["objective"]
        strategy = content["strategy"]
        execution_plan = content["execution_plan"]

        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåˆæœŸåŒ–
        self.active_testing_projects[project_id] = {
            "objective": objective,
            "strategy": strategy,
            "execution_plan": execution_plan,
            "current_step": 0,
            "status": "in_progress",
            "completed_actions": [],
            "started_at": datetime.now().isoformat(),
        }

        self.test_generation_progress[project_id] = {
            "current_action": "starting",
            "completion_percentage": 0,
            "tests_generated": 0,
            "test_files_created": 0,
        }

        # ãƒ†ã‚¹ãƒˆç”Ÿæˆé–‹å§‹
        await self._execute_testing_plan(project_id)

    async def _execute_testing_plan(self, project_id: str) -> None:
        """ãƒ†ã‚¹ãƒˆç”Ÿæˆè¨ˆç”»å®Ÿè¡Œ"""
        project = self.active_testing_projects[project_id]
        execution_plan = project["execution_plan"]

        # å„ã‚¹ãƒ†ãƒƒãƒ—ã‚’å®Ÿè¡Œ
        for step in execution_plan:
            action = step["action"]

            # é€²æ—å ±å‘Š
            await self._report_testing_progress(project_id, action, 0)

            # ã‚¢ã‚¯ã‚·ãƒ§ãƒ³å®Ÿè¡Œ
            result = await self._execute_testing_action(project_id, step)

            # çµæœå‡¦ç†
            if result.get("success", False):
                project["completed_actions"].append(action)

                # ãƒ†ã‚¹ãƒˆç”Ÿæˆçµæœå ±å‘Š
                await self._report_test_generation_result(project_id, result)

                # é€²æ—æ›´æ–°
                completion = (
                    len(project["completed_actions"]) / len(execution_plan)
                ) * 100
                await self._report_testing_progress(project_id, action, completion)

            else:
                # èª²é¡Œå ±å‘Š
                await self._report_testing_issue(
                    project_id, result.get("issue", "Unknown issue")
                )
                break

        # å…¨ã‚¹ãƒ†ãƒƒãƒ—å®Œäº†
        if len(project["completed_actions"]) == len(execution_plan):
            await self._complete_testing_project(project_id)

    async def _execute_testing_action(
        self, project_id: str, step: dict[str, Any]
    ) -> dict[str, Any]:
        """å€‹åˆ¥ãƒ†ã‚¹ãƒˆç”Ÿæˆã‚¢ã‚¯ã‚·ãƒ§ãƒ³å®Ÿè¡Œ"""
        action = step["action"]

        if action == "create_missing_test_files":
            return await self._create_missing_test_files(project_id, step)
        elif action == "generate_unit_tests":
            return await self._generate_unit_tests(project_id, step)
        elif action == "generate_edge_case_tests":
            return await self._generate_edge_case_tests(project_id, step)
        elif action == "generate_integration_tests":
            return await self._generate_integration_tests(project_id, step)
        else:
            return {"success": False, "issue": f"Unknown action: {action}"}

    async def _create_missing_test_files(
        self, project_id: str, step: dict[str, Any]
    ) -> dict[str, Any]:
        """ä¸è¶³ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ"""
        try:
            # ä¸è¶³ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç‰¹å®š
            python_files = list(Path(".").rglob("*.py"))[:10]  # æœ€åˆã®10ãƒ•ã‚¡ã‚¤ãƒ«
            python_files = [
                f
                for f in python_files
                if not any(part.startswith(".") for part in f.parts)
                and "test_" not in f.name
                and "__pycache__" not in str(f)
            ]

            test_files_created = 0

            for file_path in python_files:
                test_file = Path(f"tests/test_{file_path.stem}.py")
                if not test_file.exists():
                    test_file.parent.mkdir(parents=True, exist_ok=True)

                    # åŸºæœ¬ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ç”Ÿæˆ
                    test_content = await self._generate_basic_test_file(file_path)

                    with open(test_file, "w", encoding="utf-8") as f:
                        f.write(test_content)

                    test_files_created += 1

            return {
                "success": True,
                "test_type": "test_file_creation",
                "tests_generated": test_files_created,
                "test_files_created": test_files_created,
                "coverage_improvement": test_files_created * 5,  # ç°¡æ˜“æ¨å®š
                "completed_actions": ["create_missing_test_files"],
            }

        except Exception as e:
            return {"success": False, "issue": f"Test file creation failed: {str(e)}"}

    async def _generate_unit_tests(
        self, project_id: str, step: dict[str, Any]
    ) -> dict[str, Any]:
        """å˜ä½“ãƒ†ã‚¹ãƒˆç”Ÿæˆ"""
        try:
            # ãƒ†ã‚¹ãƒˆå¯¾è±¡é–¢æ•°ã‚’ç‰¹å®š
            functions = await self._identify_untested_functions()

            tests_generated = 0

            for func_info in functions[:8]:  # æœ€åˆã®8é–¢æ•°
                test_content = await self._generate_function_test(func_info)

                if test_content:
                    # æ—¢å­˜ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã«è¿½åŠ 
                    test_file = Path(f"tests/test_{Path(func_info['file']).stem}.py")

                    if test_file.exists():
                        with open(test_file, "a", encoding="utf-8") as f:
                            f.write(f"\n\n{test_content}")
                        tests_generated += 1

            return {
                "success": True,
                "test_type": "unit_tests",
                "tests_generated": tests_generated,
                "coverage_improvement": tests_generated * 3,  # ç°¡æ˜“æ¨å®š
                "completed_actions": ["generate_unit_tests"],
            }

        except Exception as e:
            return {"success": False, "issue": f"Unit test generation failed: {str(e)}"}

    async def _generate_edge_case_tests(
        self, project_id: str, step: dict[str, Any]
    ) -> dict[str, Any]:
        """ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹ãƒ†ã‚¹ãƒˆç”Ÿæˆ"""
        try:
            # ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹æ©Ÿä¼šã‚’ç‰¹å®š
            edge_cases = await self._identify_edge_case_opportunities()

            tests_generated = 0

            for case in edge_cases[:5]:  # æœ€åˆã®5ã‚±ãƒ¼ã‚¹
                test_content = await self._generate_edge_case_test(case)

                if test_content:
                    # å°‚ç”¨ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã«è¿½åŠ 
                    test_file = Path("tests/test_edge_cases.py")

                    if not test_file.exists():
                        test_file.parent.mkdir(parents=True, exist_ok=True)
                        with open(test_file, "w", encoding="utf-8") as f:
                            f.write('"""Edge case tests"""\nimport pytest\n\n')

                    with open(test_file, "a", encoding="utf-8") as f:
                        f.write(f"\n{test_content}")

                    tests_generated += 1

            return {
                "success": True,
                "test_type": "edge_case_tests",
                "tests_generated": tests_generated,
                "coverage_improvement": tests_generated * 2,  # ç°¡æ˜“æ¨å®š
                "completed_actions": ["generate_edge_case_tests"],
            }

        except Exception as e:
            return {
                "success": False,
                "issue": f"Edge case test generation failed: {str(e)}",
            }

    async def _generate_integration_tests(
        self, project_id: str, step: dict[str, Any]
    ) -> dict[str, Any]:
        """çµ±åˆãƒ†ã‚¹ãƒˆç”Ÿæˆ"""
        try:
            # ã‚¯ãƒ©ã‚¹ãƒ™ãƒ¼ã‚¹ã®çµ±åˆãƒ†ã‚¹ãƒˆã‚’ç”Ÿæˆ
            classes = await self._identify_testable_classes()

            tests_generated = 0

            for class_info in classes[:3]:  # æœ€åˆã®3ã‚¯ãƒ©ã‚¹
                test_content = await self._generate_class_integration_test(class_info)

                if test_content:
                    # çµ±åˆãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã«è¿½åŠ 
                    test_file = Path("tests/test_integration.py")

                    if not test_file.exists():
                        test_file.parent.mkdir(parents=True, exist_ok=True)
                        with open(test_file, "w", encoding="utf-8") as f:
                            f.write('"""Integration tests"""\nimport pytest\n\n')

                    with open(test_file, "a", encoding="utf-8") as f:
                        f.write(f"\n{test_content}")

                    tests_generated += 1

            return {
                "success": True,
                "test_type": "integration_tests",
                "tests_generated": tests_generated,
                "coverage_improvement": tests_generated * 4,  # ç°¡æ˜“æ¨å®š
                "completed_actions": ["generate_integration_tests"],
            }

        except Exception as e:
            return {
                "success": False,
                "issue": f"Integration test generation failed: {str(e)}",
            }

    async def _generate_basic_test_file(self, file_path: Path) -> str:
        """åŸºæœ¬ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ"""
        module_name = file_path.stem

        return f'''"""
Tests for {module_name}

Auto-generated by BeeKeeper Autonomous Testing Agent
"""

import pytest
from unittest.mock import Mock, patch
from {module_name} import *


class Test{module_name.title()}:
    """Test class for {module_name}"""

    def test_{module_name}_basic(self):
        """Basic functionality test"""
        # TODO: Implement actual test logic
        assert True

    def test_{module_name}_error_handling(self):
        """Error handling test"""
        # TODO: Test error conditions
        assert True
'''

    async def _identify_untested_functions(self) -> list[dict[str, Any]]:
        """ãƒ†ã‚¹ãƒˆæœªå®Ÿè£…é–¢æ•°ç‰¹å®š"""
        functions = []

        python_files = list(Path(".").rglob("*.py"))[:8]
        python_files = [
            f
            for f in python_files
            if not any(part.startswith(".") for part in f.parts)
            and "test_" not in f.name
            and "__pycache__" not in str(f)
        ]

        for file_path in python_files:
            try:
                with open(file_path, encoding="utf-8") as f:
                    tree = ast.parse(f.read())

                for node in ast.walk(tree):
                    if isinstance(node, ast.FunctionDef):
                        if not node.name.startswith("_"):
                            functions.append(
                                {
                                    "name": node.name,
                                    "file": str(file_path),
                                    "args": [arg.arg for arg in node.args.args],
                                }
                            )

            except Exception:
                continue

        return functions

    async def _generate_function_test(self, func_info: dict[str, Any]) -> str:
        """é–¢æ•°ãƒ†ã‚¹ãƒˆç”Ÿæˆ"""
        func_name = func_info["name"]
        args = func_info["args"]

        # ç°¡æ˜“ãƒ†ã‚¹ãƒˆç”Ÿæˆ
        return f'''def test_{func_name}():
    """Test {func_name} function"""
    # TODO: Implement comprehensive test for {func_name}
    # Args: {args}
    result = {func_name}({self._generate_sample_args(args)})
    assert result is not None'''

    def _generate_sample_args(self, args: list[str]) -> str:
        """ã‚µãƒ³ãƒ—ãƒ«å¼•æ•°ç”Ÿæˆ"""
        if not args:
            return ""

        sample_args = []
        for arg in args:
            if arg == "self":
                continue
            sample_args.append(f'"{arg}_value"')

        return ", ".join(sample_args)

    async def _identify_edge_case_opportunities(self) -> list[dict[str, Any]]:
        """ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹æ©Ÿä¼šç‰¹å®š"""
        return [
            {"type": "null_input", "description": "Test with None input"},
            {"type": "empty_string", "description": "Test with empty string"},
            {"type": "zero_value", "description": "Test with zero value"},
            {"type": "negative_value", "description": "Test with negative value"},
            {"type": "large_value", "description": "Test with large value"},
        ]

    async def _generate_edge_case_test(self, case: dict[str, Any]) -> str:
        """ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹ãƒ†ã‚¹ãƒˆç”Ÿæˆ"""
        case_type = case["type"]
        description = case["description"]

        return f'''def test_edge_case_{case_type}():
    """Edge case test: {description}"""
    # TODO: Implement edge case test for {case_type}
    assert True  # Placeholder'''

    async def _identify_testable_classes(self) -> list[dict[str, Any]]:
        """ãƒ†ã‚¹ãƒˆå¯¾è±¡ã‚¯ãƒ©ã‚¹ç‰¹å®š"""
        classes = []

        python_files = list(Path(".").rglob("*.py"))[:5]
        python_files = [
            f
            for f in python_files
            if not any(part.startswith(".") for part in f.parts)
            and "test_" not in f.name
            and "__pycache__" not in str(f)
        ]

        for file_path in python_files:
            try:
                with open(file_path, encoding="utf-8") as f:
                    tree = ast.parse(f.read())

                for node in ast.walk(tree):
                    if isinstance(node, ast.ClassDef):
                        classes.append({"name": node.name, "file": str(file_path)})

            except Exception:
                continue

        return classes

    async def _generate_class_integration_test(self, class_info: dict[str, Any]) -> str:
        """ã‚¯ãƒ©ã‚¹çµ±åˆãƒ†ã‚¹ãƒˆç”Ÿæˆ"""
        class_name = class_info["name"]

        return f'''def test_{class_name.lower()}_integration():
    """Integration test for {class_name}"""
    # TODO: Implement integration test for {class_name}
    instance = {class_name}()
    assert instance is not None'''

    async def _report_testing_progress(
        self, project_id: str, action: str, completion: float
    ) -> None:
        """ãƒ†ã‚¹ãƒˆç”Ÿæˆé€²æ—å ±å‘Š"""
        progress_update = {
            "type": "testing_progress",
            "project_id": project_id,
            "progress": {
                "current_action": action,
                "completion_percentage": completion,
                "description": f"Executing {action}",
                "timestamp": datetime.now().isoformat(),
            },
        }

        self.comb_api.send_message(
            to_worker="queen",
            content=progress_update,
            message_type=MessageType.NOTIFICATION,
            priority=MessagePriority.MEDIUM,
        )

    async def _report_test_generation_result(
        self, project_id: str, result: dict[str, Any]
    ) -> None:
        """ãƒ†ã‚¹ãƒˆç”Ÿæˆçµæœå ±å‘Š"""
        result_report = {
            "type": "test_generation_result",
            "project_id": project_id,
            "result": result,
            "timestamp": datetime.now().isoformat(),
        }

        self.comb_api.send_message(
            to_worker="queen",
            content=result_report,
            message_type=MessageType.NOTIFICATION,
            priority=MessagePriority.HIGH,
        )

    async def _report_testing_issue(self, project_id: str, issue: str) -> None:
        """ãƒ†ã‚¹ãƒˆç”Ÿæˆèª²é¡Œå ±å‘Š"""
        issue_report = {
            "type": "testing_issue",
            "project_id": project_id,
            "issue": issue,
            "severity": "medium",
            "timestamp": datetime.now().isoformat(),
        }

        self.comb_api.send_message(
            to_worker="queen",
            content=issue_report,
            message_type=MessageType.URGENT_NOTIFICATION,
            priority=MessagePriority.HIGH,
        )

    async def _complete_testing_project(self, project_id: str) -> None:
        """ãƒ†ã‚¹ãƒˆç”Ÿæˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå®Œäº†"""
        project = self.active_testing_projects[project_id]
        progress = self.test_generation_progress[project_id]

        # æœ€çµ‚ã‚«ãƒãƒ¬ãƒƒã‚¸è¨ˆç®—
        final_coverage = await self._calculate_final_coverage()

        final_results = {
            "total_tests_generated": progress["tests_generated"],
            "test_files_created": progress["test_files_created"],
            "final_coverage": final_coverage,
            "coverage_improvement": final_coverage - 0,  # ç°¡æ˜“è¨ˆç®—
            "actions_completed": len(project["completed_actions"]),
            "total_actions": len(project["execution_plan"]),
        }

        # ãƒ†ã‚¹ãƒˆç¨®åˆ¥ã‚µãƒãƒªãƒ¼
        test_summary = {
            "unit_tests": progress["tests_generated"] // 2,
            "edge_case_tests": progress["tests_generated"] // 4,
            "integration_tests": progress["tests_generated"] // 4,
        }

        completion_report = {
            "type": "testing_completion",
            "project_id": project_id,
            "final_results": final_results,
            "test_summary": test_summary,
            "summary": f"Testing completed successfully. Generated {final_results['total_tests_generated']} tests, coverage: {final_coverage:.1f}%",
            "timestamp": datetime.now().isoformat(),
        }

        self.comb_api.send_message(
            to_worker="queen",
            content=completion_report,
            message_type=MessageType.NOTIFICATION,
            priority=MessagePriority.HIGH,
        )

        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆçµ‚äº†
        project["status"] = "completed"
        project["completed_at"] = datetime.now().isoformat()

    async def _calculate_final_coverage(self) -> float:
        """æœ€çµ‚ã‚«ãƒãƒ¬ãƒƒã‚¸è¨ˆç®—"""
        try:
            result = subprocess.run(
                ["python", "-m", "pytest", "--cov=.", "--cov-report=json", "--quiet"],
                capture_output=True,
                text=True,
                timeout=60,
            )

            if result.returncode == 0:
                coverage_file = Path("coverage.json")
                if coverage_file.exists():
                    with open(coverage_file) as f:
                        data = json.load(f)
                    coverage_file.unlink()

                    return data.get("totals", {}).get("percent_covered", 0)

            return 0.0

        except Exception:
            return 0.0

    async def _receive_testing_guidance(self, content: dict[str, Any]) -> None:
        """ãƒ†ã‚¹ãƒˆç”ŸæˆæŒ‡å°å—ä¿¡"""
        guidance = content.get("guidance", "")
        self.logger.info(f"ğŸ’» Developer: Received guidance: {guidance}")

    async def _continue_testing(self, content: dict[str, Any]) -> None:
        """ãƒ†ã‚¹ãƒˆç”Ÿæˆç¶™ç¶š"""
        next_action = content.get("next_action", {})
        self.logger.info(f"ğŸ’» Developer: Continuing with: {next_action}")

    async def _receive_testing_answer(self, content: dict[str, Any]) -> None:
        """ãƒ†ã‚¹ãƒˆç”Ÿæˆå›ç­”å—ä¿¡"""
        answer = content.get("answer", "")
        self.logger.info(f"ğŸ’» Developer: Received answer: {answer}")

    async def _receive_testing_solution(self, content: dict[str, Any]) -> None:
        """ãƒ†ã‚¹ãƒˆç”Ÿæˆè§£æ±ºç­–å—ä¿¡"""
        solution = content.get("solution", "")
        self.logger.info(f"ğŸ’» Developer: Received solution: {solution}")


# çµ±åˆãƒ‡ãƒ¢å®Ÿè¡Œ


async def demo_beekeeper_testing():
    """BeeKeeper ãƒ†ã‚¹ãƒˆç”Ÿæˆãƒ‡ãƒ¢"""

    print("ğŸ Starting BeeKeeper Autonomous Testing Demo")
    print("=" * 50)

    # 1. BeeKeeper å…¥åŠ›
    beekeeper = BeeKeeperTestingInput()
    request_id = beekeeper.submit_testing_request(
        objective="Generate comprehensive test suite for improved code coverage",
        template="Analyze codebase â†’ Create missing test files â†’ Generate unit tests â†’ Add edge case tests â†’ Validate coverage improvement",
        target_files=[],
        coverage_targets={
            "line_coverage": 85,
            "branch_coverage": 80,
            "function_coverage": 90,
            "edge_cases_coverage": 75,
        },
        test_types=["unit", "edge_case", "integration"],
    )

    # 2. Queen Coordinator åˆæœŸåŒ–
    queen = QueenTestingCoordinator()

    # 3. Developer Worker åˆæœŸåŒ–
    developer = DeveloperTestingWorker()

    # 4. Developer Worker ç›£è¦–é–‹å§‹
    developer_task = asyncio.create_task(developer.start_testing_monitoring())

    # 5. Queen ãŒ BeeKeeper å…¥åŠ›ã‚’å‡¦ç†
    request_data = beekeeper.get_request_data(request_id)
    await queen.receive_beekeeper_request(request_id, request_data)

    # 6. Queen-Developer å”èª¿ç›£è¦–
    print("\nğŸ”„ Queen-Developer testing collaboration started...")

    project_id = f"testing_{request_id}"
    for _cycle in range(10):  # 10ã‚µã‚¤ã‚¯ãƒ«ç›£è¦–
        await queen.monitor_testing_progress(project_id)
        await asyncio.sleep(12)  # 12ç§’é–“éš”

        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå®Œäº†ãƒã‚§ãƒƒã‚¯
        if project_id in queen.active_testing_projects:
            if queen.active_testing_projects[project_id]["status"] == "completed":
                print(f"âœ… Testing project {project_id} completed!")
                break

    # 7. çµ‚äº†å‡¦ç†
    developer_task.cancel()

    # 8. çµæœè¡¨ç¤º
    print("\nğŸ“Š Testing Results:")
    print(f"ğŸ—‚ï¸  Conversation History: {len(queen.conversation_history)} messages")
    print(f"ğŸ“ Active Projects: {len(queen.active_testing_projects)}")

    # æˆæœç‰©ç¢ºèª
    honey_dir = Path(f".hive/honey/beekeeper_testing/{project_id}")
    if honey_dir.exists():
        print(f"ğŸ¯ Testing deliverables saved to: {honey_dir}")
        for file in honey_dir.glob("*"):
            print(f"   - {file.name}")

    print("\nğŸ‰ BeeKeeper Testing Demo Completed!")


if __name__ == "__main__":
    # ãƒ­ã‚°è¨­å®š
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    )

    # ãƒ‡ãƒ¢å®Ÿè¡Œ
    asyncio.run(demo_beekeeper_testing())
